{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# LSTM (Long Short Term Memory) for title generation based on YouTube blogger's chanel"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import re\n",
    "from pytube import YouTube\n",
    "from pytube import Playlist\n",
    "from pytube import Channel\n",
    "\n",
    "# keras module for building LSTM \n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "import keras.utils as ku \n",
    "\n",
    "# set seeds for reproducability\n",
    "# from tensorflow import set_random_seed\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string, os \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Init Plugin\n",
      "Init Graph Optimizer\n",
      "Init Kernel\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "source": [
    "# all_headlines = []\n",
    "\n",
    "# playlist = Playlist(\"https://www.youtube.com/watch?v=6I2L7qORwN4&list=PLVqpOPG2Lex_htBmJFI0SMfU44S6msx5r\")\n",
    "# playlist._video_regex = re.compile(r\"\\\"url\\\":\\\"(/watch\\?v=[\\w-]*)\")\n",
    "# print('Number of videos in playlist: %s' % len(playlist.video_urls))\n",
    "# for url in playlist.video_urls:\n",
    "#     yt = YouTube(url)\n",
    "#     all_headlines.append(yt.title[:-29])\n",
    "\n",
    "# all_headlines[:10]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "c = Channel('https://www.youtube.com/c/maxkatz1')\n",
    "print(f'Downloading videos by: {c.channel_name}')\n",
    "print('Всего видео на канале: ', len(c.videos))\n",
    "all_headlines = []\n",
    "for video in c.videos:\n",
    "    all_headlines.append(video.title)\n",
    "all_headlines[:7]"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading videos by: Максим Кац\n",
      "Всего видео на канале:  721\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['Доллар дешевеет, цены растут. Что будет дальше с Россией? (English subs) / @Максим Кац',\n",
       " 'Бурятия, Дагестан и другие регионы на убой. Чьими руками воюет Путин (English subs) / @Максим Кац',\n",
       " 'Как правильно дезактивировать пропаганду. Мнение (English subs) / @Максим Кац',\n",
       " 'Путин уничтожает Россию. Не будьте частью этого (English subs) / @Максим Кац',\n",
       " 'Чего добился Путин. Итоги войны России против Украины (English subs) / @Максим Кац',\n",
       " 'Противостояние Путина с НАТО (English subs) / @Максим Кац',\n",
       " 'Война 2022. Когда всё началось (English subs) / @Максим Кац']"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "def clean_text(txt):\n",
    "    txt = \"\".join(v for v in txt if v not in string.punctuation).lower()\n",
    "    return txt \n",
    "    \n",
    "corpus = [clean_text(x) for x in all_headlines]\n",
    "corpus[:7]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['доллар дешевеет цены растут что будет дальше с россией english subs  максим кац',\n",
       " 'бурятия дагестан и другие регионы на убой чьими руками воюет путин english subs  максим кац',\n",
       " 'как правильно дезактивировать пропаганду мнение english subs  максим кац',\n",
       " 'путин уничтожает россию не будьте частью этого english subs  максим кац',\n",
       " 'чего добился путин итоги войны россии против украины english subs  максим кац',\n",
       " 'противостояние путина с нато english subs  максим кац',\n",
       " 'война 2022 когда всё началось english subs  максим кац']"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# Tokenization is a process of extracting tokens (terms / words) from a corpus. \n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "def get_sequence_of_tokens(corpus):\n",
    "    ## tokenization\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    total_words = len(tokenizer.word_index) + 1\n",
    "    \n",
    "    ## convert data to sequence of tokens \n",
    "    input_sequences = []\n",
    "    for line in corpus:\n",
    "        token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "        for i in range(1, len(token_list)):\n",
    "            n_gram_sequence = token_list[:i+1]\n",
    "            input_sequences.append(n_gram_sequence)\n",
    "    return input_sequences, total_words\n",
    "\n",
    "\n",
    "# In the above output [30, 507], [30, 507, 11], [30, 507, 11, 1] and so on \n",
    "# represents the ngram phrases generated from the input data. \n",
    "# every integer corresponds to the index of a particular word in the complete vocabulary of words present in the text. \n",
    "# For example\n",
    "inp_sequences, total_words = get_sequence_of_tokens(corpus)\n",
    "inp_sequences[:10]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[583, 584],\n",
       " [583, 584, 195],\n",
       " [583, 584, 195, 301],\n",
       " [583, 584, 195, 301, 7],\n",
       " [583, 584, 195, 301, 7, 47],\n",
       " [583, 584, 195, 301, 7, 47, 52],\n",
       " [583, 584, 195, 301, 7, 47, 52, 6],\n",
       " [583, 584, 195, 301, 7, 47, 52, 6, 196],\n",
       " [583, 584, 195, 301, 7, 47, 52, 6, 196, 10],\n",
       " [583, 584, 195, 301, 7, 47, 52, 6, 196, 10, 11]]"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "from keras.utils import np_utils\n",
    "\n",
    "def generate_padded_sequences(input_sequences):\n",
    "    max_sequence_len = max([len(x) for x in input_sequences])\n",
    "    input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "    \n",
    "    predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "    label = ku.np_utils.to_categorical(label, num_classes=total_words)\n",
    "    return predictors, label, max_sequence_len\n",
    "\n",
    "predictors, label, max_sequence_len = generate_padded_sequences(inp_sequences)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def create_model(max_sequence_len, total_words):\n",
    "    input_len = max_sequence_len - 1\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Add Input Embedding Layer\n",
    "    model.add(Embedding(total_words, 10, input_length=input_len))\n",
    "    \n",
    "    # Add Hidden Layer 1 - LSTM Layer\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dropout(0.1))\n",
    "    \n",
    "    # Add Output Layer\n",
    "    model.add(Dense(total_words, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = create_model(max_sequence_len, total_words)\n",
    "model.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Metal device set to: Apple M1\n",
      "\n",
      "systemMemory: 8.00 GB\n",
      "maxCacheSize: 2.67 GB\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2022-05-22 17:31:08.747676: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-05-22 17:31:08.748985: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 16, 10)            25180     \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 100)               44400     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2518)              254318    \n",
      "=================================================================\n",
      "Total params: 323,898\n",
      "Trainable params: 323,898\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "model.fit(predictors, label, epochs=150, verbose=5)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/150\n",
      "Epoch 2/150\n",
      "Epoch 3/150\n",
      "Epoch 4/150\n",
      "Epoch 5/150\n",
      "Epoch 6/150\n",
      "Epoch 7/150\n",
      "Epoch 8/150\n",
      "Epoch 9/150\n",
      "Epoch 10/150\n",
      "Epoch 11/150\n",
      "Epoch 12/150\n",
      "Epoch 13/150\n",
      "Epoch 14/150\n",
      "Epoch 15/150\n",
      "Epoch 16/150\n",
      "Epoch 17/150\n",
      "Epoch 18/150\n",
      "Epoch 19/150\n",
      "Epoch 20/150\n",
      "Epoch 21/150\n",
      "Epoch 22/150\n",
      "Epoch 23/150\n",
      "Epoch 24/150\n",
      "Epoch 25/150\n",
      "Epoch 26/150\n",
      "Epoch 27/150\n",
      "Epoch 28/150\n",
      "Epoch 29/150\n",
      "Epoch 30/150\n",
      "Epoch 31/150\n",
      "Epoch 32/150\n",
      "Epoch 33/150\n",
      "Epoch 34/150\n",
      "Epoch 35/150\n",
      "Epoch 36/150\n",
      "Epoch 37/150\n",
      "Epoch 38/150\n",
      "Epoch 39/150\n",
      "Epoch 40/150\n",
      "Epoch 41/150\n",
      "Epoch 42/150\n",
      "Epoch 43/150\n",
      "Epoch 44/150\n",
      "Epoch 45/150\n",
      "Epoch 46/150\n",
      "Epoch 47/150\n",
      "Epoch 48/150\n",
      "Epoch 49/150\n",
      "Epoch 50/150\n",
      "Epoch 51/150\n",
      "Epoch 52/150\n",
      "Epoch 53/150\n",
      "Epoch 54/150\n",
      "Epoch 55/150\n",
      "Epoch 56/150\n",
      "Epoch 57/150\n",
      "Epoch 58/150\n",
      "Epoch 59/150\n",
      "Epoch 60/150\n",
      "Epoch 61/150\n",
      "Epoch 62/150\n",
      "Epoch 63/150\n",
      "Epoch 64/150\n",
      "Epoch 65/150\n",
      "Epoch 66/150\n",
      "Epoch 67/150\n",
      "Epoch 68/150\n",
      "Epoch 69/150\n",
      "Epoch 70/150\n",
      "Epoch 71/150\n",
      "Epoch 72/150\n",
      "Epoch 73/150\n",
      "Epoch 74/150\n",
      "Epoch 75/150\n",
      "Epoch 76/150\n",
      "Epoch 77/150\n",
      "Epoch 78/150\n",
      "Epoch 79/150\n",
      "Epoch 80/150\n",
      "Epoch 81/150\n",
      "Epoch 82/150\n",
      "Epoch 83/150\n",
      "Epoch 84/150\n",
      "Epoch 85/150\n",
      "Epoch 86/150\n",
      "Epoch 87/150\n",
      "Epoch 88/150\n",
      "Epoch 89/150\n",
      "Epoch 90/150\n",
      "Epoch 91/150\n",
      "Epoch 92/150\n",
      "Epoch 93/150\n",
      "Epoch 94/150\n",
      "Epoch 95/150\n",
      "Epoch 96/150\n",
      "Epoch 97/150\n",
      "Epoch 98/150\n",
      "Epoch 99/150\n",
      "Epoch 100/150\n",
      "Epoch 101/150\n",
      "Epoch 102/150\n",
      "Epoch 103/150\n",
      "Epoch 104/150\n",
      "Epoch 105/150\n",
      "Epoch 106/150\n",
      "Epoch 107/150\n",
      "Epoch 108/150\n",
      "Epoch 109/150\n",
      "Epoch 110/150\n",
      "Epoch 111/150\n",
      "Epoch 112/150\n",
      "Epoch 113/150\n",
      "Epoch 114/150\n",
      "Epoch 115/150\n",
      "Epoch 116/150\n",
      "Epoch 117/150\n",
      "Epoch 118/150\n",
      "Epoch 119/150\n",
      "Epoch 120/150\n",
      "Epoch 121/150\n",
      "Epoch 122/150\n",
      "Epoch 123/150\n",
      "Epoch 124/150\n",
      "Epoch 125/150\n",
      "Epoch 126/150\n",
      "Epoch 127/150\n",
      "Epoch 128/150\n",
      "Epoch 129/150\n",
      "Epoch 130/150\n",
      "Epoch 131/150\n",
      "Epoch 132/150\n",
      "Epoch 133/150\n",
      "Epoch 134/150\n",
      "Epoch 135/150\n",
      "Epoch 136/150\n",
      "Epoch 137/150\n",
      "Epoch 138/150\n",
      "Epoch 139/150\n",
      "Epoch 140/150\n",
      "Epoch 141/150\n",
      "Epoch 142/150\n",
      "Epoch 143/150\n",
      "Epoch 144/150\n",
      "Epoch 145/150\n",
      "Epoch 146/150\n",
      "Epoch 147/150\n",
      "Epoch 148/150\n",
      "Epoch 149/150\n",
      "Epoch 150/150\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x298c580a0>"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "source": [
    "def generate_text(seed_text, next_words, model, max_sequence_len):\n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "        predicted = model.predict_classes(token_list, verbose=0)\n",
    "        \n",
    "        output_word = \"\"\n",
    "        for word,index in tokenizer.word_index.items():\n",
    "            if index == predicted:\n",
    "                output_word = word\n",
    "                break\n",
    "        seed_text += \" \"+output_word\n",
    "    return seed_text.title()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "source": [
    "print (generate_text(\"Без Путина\", 13, model, max_sequence_len))\n",
    "print (generate_text(\"что будет дальше\", 15, model, max_sequence_len))\n",
    "print (generate_text(\"Противостояние Путина с\", 13, model, max_sequence_len))\n",
    "print (generate_text(\"Навального\", 14, model, max_sequence_len))\n",
    "print (generate_text(\"Может ли\", 15, model, max_sequence_len).split()[-1])\n",
    "print (generate_text(\"Что будет\", 15, model, max_sequence_len))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Без Путина И России — В Тельавива Имперских Без Надо English Subs Максим Кац ​\n",
      "Что Будет Дальше Россия И России — И Больше Имперских Замашек Надо Не Надо English Subs Максим Кац\n",
      "Противостояние Путина С Нато Максим Кац Навальный Максим Кац Из Максим Кац Августа Манижа И Россия\n",
      "Навального И Россия И Россия — России В Эфире Тельавива Где English Subs Максим Кац\n",
      "Кац\n",
      "Что Будет И Россия — России — И Больше Имперских Без Без English Subs Максим Кац ​\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "source": [
    "seed_w = set()\n",
    "for h in corpus:\n",
    "    seed_w.add(h.split()[0])\n",
    "seed_w = list(seed_w)\n",
    "print(seed_w[:10])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['форд', 'джордж', 'куда', 'палестиноизраильский', 'пражская', 'боятся', 'прогресс', 'смерть', 'израиль', 'маргарет']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "source": [
    "HeadLines = []\n",
    "for w in seed_w:\n",
    "    h = generate_text(w, max_sequence_len, model, max_sequence_len)\n",
    "    if 'English Subs Максим Кац' in h:\n",
    "        pos = h.split().index('Кац')\n",
    "        t = str()\n",
    "        for hh in h.split()[:pos+1] : t+=(hh+' ')\n",
    "        HeadLines.append(t)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "source": [
    "len(HeadLines)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "351"
      ]
     },
     "metadata": {},
     "execution_count": 204
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "source": [
    "f = open('best_headings.txt', 'w')\n",
    "for h in HeadLines:\n",
    "    f.write(h+'\\n')\n",
    "f.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Some results "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ссср Как Как Не Надо English Subs Максим Кац \n",
    "\n",
    "Диктатура И Россия — России — И Больше Имперских Без Без English Subs Максим Кац\n",
    "\n",
    "Хрущевки И Вашем — Россия И России С English Subs Максим Кац \n",
    "\n",
    "Многоэтажки И России — И Больше Имперских Замашек Без Не Надо English Subs Максим Кац\n",
    "\n",
    "Голосовать И Россия И Россия — России В Эфире Тельавива Где English Subs Максим Кац\n",
    "\n",
    "Очереди И Россия — России — И Больше Имперских Без Без Не English Subs Максим Кац \n",
    "\n",
    "Мир И Вашем — Россия И России С English Subs Максим Кац \n",
    "\n",
    "Образование И России — И Больше Имперских Замашек Надо Не Надо English Subs Максим Кац \n",
    "\n",
    "Иллюзия Как Как Как Не Надо English Subs Максим Кац \n",
    "\n",
    "Разоблачения Как Как Как Не Надо English Subs Максим Кац \n",
    "\n",
    "Народный Не Надо English Subs Максим Кац \n",
    "\n",
    "Ложь И Россия — России Как English Subs Максим Кац \n",
    "\n",
    "Отрицательный Как Как Как Не Надо English Subs Максим Кац \n",
    "\n",
    "Российские И Россия — России Как English Subs Максим Кац \n",
    "\n",
    "Саакашвили Не Надо English Subs Максим Кац \n",
    "\n",
    "Евросоюз И России Как English Subs Максим Кац \n",
    "\n",
    "Ужасы И Россия — России Как English Subs Максим Кац \n",
    "\n",
    "​Максим И Россия — России Как English Subs Максим Кац \n",
    "\n",
    "Буча Как Как Как Не Надо English Subs Максим Кац \n",
    "\n",
    "Революция И Россия — России Как English Subs Максим Кац "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.12 64-bit ('tensorflow': conda)"
  },
  "interpreter": {
   "hash": "0846c2e6f4b8bd8319dd4a6ed0143631adec4ae6e3ed4669d5338364814320e2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}